{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11450543,"sourceType":"datasetVersion","datasetId":7116867}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:23.364858Z","iopub.execute_input":"2025-04-17T16:13:23.365867Z","iopub.status.idle":"2025-04-17T16:13:23.369282Z","shell.execute_reply.started":"2025-04-17T16:13:23.365835Z","shell.execute_reply":"2025-04-17T16:13:23.368568Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ndata_dir = \"/kaggle/input/characters-data/dataset\"\n\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset,[train_size, val_size, test_size])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:23.370467Z","iopub.execute_input":"2025-04-17T16:13:23.370707Z","iopub.status.idle":"2025-04-17T16:13:58.963219Z","shell.execute_reply.started":"2025-04-17T16:13:23.370683Z","shell.execute_reply":"2025-04-17T16:13:58.962398Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(dataset.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:11:24.486722Z","iopub.execute_input":"2025-04-17T17:11:24.487290Z","iopub.status.idle":"2025-04-17T17:11:24.491285Z","shell.execute_reply.started":"2025-04-17T17:11:24.487255Z","shell.execute_reply":"2025-04-17T17:11:24.490539Z"}},"outputs":[{"name":"stdout","text":"{'digit_0': 0, 'digit_1': 1, 'digit_2': 2, 'digit_3': 3, 'digit_4': 4, 'digit_5': 5, 'digit_6': 6, 'digit_7': 7, 'digit_8': 8, 'digit_9': 9, 'lower_a': 10, 'lower_b': 11, 'lower_c': 12, 'lower_d': 13, 'lower_e': 14, 'lower_f': 15, 'lower_g': 16, 'lower_h': 17, 'lower_i': 18, 'lower_j': 19, 'lower_k': 20, 'lower_l': 21, 'lower_m': 22, 'lower_n': 23, 'lower_o': 24, 'lower_p': 25, 'lower_q': 26, 'lower_r': 27, 'lower_s': 28, 'lower_t': 29, 'lower_u': 30, 'lower_v': 31, 'lower_w': 32, 'lower_x': 33, 'lower_y': 34, 'lower_z': 35, 'upper_A': 36, 'upper_B': 37, 'upper_C': 38, 'upper_D': 39, 'upper_E': 40, 'upper_F': 41, 'upper_G': 42, 'upper_H': 43, 'upper_I': 44, 'upper_J': 45, 'upper_K': 46, 'upper_L': 47, 'upper_M': 48, 'upper_N': 49, 'upper_O': 50, 'upper_P': 51, 'upper_Q': 52, 'upper_R': 53, 'upper_S': 54, 'upper_T': 55, 'upper_U': 56, 'upper_V': 57, 'upper_W': 58, 'upper_X': 59, 'upper_Y': 60, 'upper_Z': 61}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:58.964064Z","iopub.execute_input":"2025-04-17T16:13:58.964464Z","iopub.status.idle":"2025-04-17T16:13:58.968838Z","shell.execute_reply.started":"2025-04-17T16:13:58.964436Z","shell.execute_reply":"2025-04-17T16:13:58.968121Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# ðŸ“Œ Preparing the model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        self.block1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        self.block2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        self.fc1 = nn.Linear(64 * 35 * 35, 64)\n        self.fc2 = nn.Linear(64, 62)\n\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        \n        x = torch.flatten(x, 1)\n\n        x = self.dropout(self.fc1(x))\n        x = self.fc2(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:58.970676Z","iopub.execute_input":"2025-04-17T16:13:58.970944Z","iopub.status.idle":"2025-04-17T16:13:58.988259Z","shell.execute_reply.started":"2025-04-17T16:13:58.970917Z","shell.execute_reply":"2025-04-17T16:13:58.987628Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = CNN()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:58.989054Z","iopub.execute_input":"2025-04-17T16:13:58.989333Z","iopub.status.idle":"2025-04-17T16:13:59.315007Z","shell.execute_reply.started":"2025-04-17T16:13:58.989309Z","shell.execute_reply":"2025-04-17T16:13:59.314379Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (block1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=78400, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=62, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:59.315684Z","iopub.execute_input":"2025-04-17T16:13:59.315917Z","iopub.status.idle":"2025-04-17T16:13:59.320603Z","shell.execute_reply.started":"2025-04-17T16:13:59.315899Z","shell.execute_reply":"2025-04-17T16:13:59.319795Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# ðŸ“Œ Training","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“Œ Testing","metadata":{}},{"cell_type":"code","source":"num_epochs = 20\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)  # force it to GPU\n        \n        outputs = model(images)  # Forward pass\n        loss = criterion(outputs, labels)  # loss function\n        \n        optimizer.zero_grad()  # Reset gradients\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n        \n        running_loss += loss.item()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    scheduler.step()  # Adjust learning rate\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total  # Calculate training accuracy\n\n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_acc)\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # Disable gradients computations\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            \n            _ , predicted = torch.max(outputs, 1)  # Get predicted labels\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_loss /= len(val_loader)\n    val_accuracy = 100 * correct / total  # Calculate accuracy\n\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n\n    print(f\"Epoch [{epoch+1:2d}/{num_epochs:2d}] |   \"\n          f\"Train Loss: {epoch_loss:8.4f}, Train Acc: {epoch_acc*100:5.2f}%   |   \"\n          f\"Val Loss: {val_loss:8.4f}, Val Acc: {val_accuracy:5.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:13:59.321262Z","iopub.execute_input":"2025-04-17T16:13:59.321532Z"}},"outputs":[{"name":"stdout","text":"Epoch [ 1/20] |   Train Loss:   1.8722, Train Acc: 62.66%   |   Val Loss:   0.5212, Val Acc: 87.48%\nEpoch [ 2/20] |   Train Loss:   0.4240, Train Acc: 88.49%   |   Val Loss:   0.3208, Val Acc: 91.77%\nEpoch [ 3/20] |   Train Loss:   0.2370, Train Acc: 93.00%   |   Val Loss:   0.3491, Val Acc: 91.16%\nEpoch [ 4/20] |   Train Loss:   0.1601, Train Acc: 95.09%   |   Val Loss:   0.3124, Val Acc: 91.99%\nEpoch [ 5/20] |   Train Loss:   0.1251, Train Acc: 96.12%   |   Val Loss:   0.2321, Val Acc: 94.67%\nEpoch [ 6/20] |   Train Loss:   0.0693, Train Acc: 97.79%   |   Val Loss:   0.2263, Val Acc: 95.01%\nEpoch [ 7/20] |   Train Loss:   0.0612, Train Acc: 98.04%   |   Val Loss:   0.2112, Val Acc: 95.39%\nEpoch [ 8/20] |   Train Loss:   0.0503, Train Acc: 98.28%   |   Val Loss:   0.6678, Val Acc: 86.86%\nEpoch [ 9/20] |   Train Loss:   0.0436, Train Acc: 98.63%   |   Val Loss:   0.2176, Val Acc: 95.41%\nEpoch [10/20] |   Train Loss:   0.0408, Train Acc: 98.64%   |   Val Loss:   0.2022, Val Acc: 96.09%\nEpoch [11/20] |   Train Loss:   0.0253, Train Acc: 99.15%   |   Val Loss:   0.1886, Val Acc: 96.73%\nEpoch [12/20] |   Train Loss:   0.0183, Train Acc: 99.38%   |   Val Loss:   0.2146, Val Acc: 96.65%\nEpoch [13/20] |   Train Loss:   0.0189, Train Acc: 99.38%   |   Val Loss:   0.1836, Val Acc: 96.69%\nEpoch [14/20] |   Train Loss:   0.0200, Train Acc: 99.38%   |   Val Loss:   0.2031, Val Acc: 96.77%\nEpoch [15/20] |   Train Loss:   0.0138, Train Acc: 99.54%   |   Val Loss:   1.0523, Val Acc: 83.17%\nEpoch [16/20] |   Train Loss:   0.0125, Train Acc: 99.58%   |   Val Loss:   0.2063, Val Acc: 97.01%\nEpoch [17/20] |   Train Loss:   0.0090, Train Acc: 99.72%   |   Val Loss:   0.2057, Val Acc: 96.91%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntest_loss = 0.0\ntest_correct = 0\ntest_total = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        test_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_loss /= len(test_loader)\ntest_accuracy = 100 * test_correct / test_total\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:53:46.593963Z","iopub.execute_input":"2025-04-17T16:53:46.594590Z","iopub.status.idle":"2025-04-17T16:54:11.577372Z","shell.execute_reply.started":"2025-04-17T16:53:46.594568Z","shell.execute_reply":"2025-04-17T16:54:11.576595Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 0.2763, Test Accuracy: 96.85%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nloss_df = pd.DataFrame({\n    \"Epoch\": list(range(1, num_epochs + 1)) * 2,\n    \"Loss\": train_losses + val_losses,\n    \"Type\": [\"Train\"] * num_epochs + [\"Validation\"] * num_epochs\n})\n\nsns.set(style=\"whitegrid\")\nsns.lineplot(data=loss_df, x=\"Epoch\", y=\"Loss\", hue=\"Type\", marker=\"o\")\n\nplt.xticks(range(1, num_epochs + 1, 2))\nplt.title(\"Training vs Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(title=\"Loss Type\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T16:55:02.425973Z","iopub.execute_input":"2025-04-17T16:55:02.426258Z","iopub.status.idle":"2025-04-17T16:55:02.465180Z","shell.execute_reply.started":"2025-04-17T16:55:02.426234Z","shell.execute_reply":"2025-04-17T16:55:02.464419Z"}},"outputs":[],"execution_count":11}]}